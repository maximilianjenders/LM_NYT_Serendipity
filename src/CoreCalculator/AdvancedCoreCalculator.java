package CoreCalculator;

import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Random;
import java.util.concurrent.Executors;

import util.DBConnection;
import util.Helper;

import dataclass.Article;
import dataclass.ArticleFeature;
import dataclass.ArticleFeatureMaintainer;

/***
 * Implementations with more advanced models (LMs for initial article and article most different)
 * @author Max
 *
 */
public class AdvancedCoreCalculator extends CoreCalculator {

	private HashSet<Article> thetaPlusArticles;
	private HashSet<Article> thetaMinusArticles;
	//the number of seed documents to choose in order to build theta minus
	private final int NUMSEEDSTHETAMINUS = 50;
	//Probabilities only used for entropy calculation, since theta has to be smooothed for prob calculation
	private HashMap<Integer, Integer> thetaPlus; //map: FeatureID -> count
	private HashMap<Integer, Integer> thetaMinus; //map: FeatureID -> prob.
	boolean advancedAlgorithm = true;
	

	
	
	public AdvancedCoreCalculator(ArticleFeatureMaintainer afm) {
		//Instantiate a FeatureMaintainer with all documents 
		fm = afm;
		//Initialize theta+ with the inital document
		thetaPlusArticles = new HashSet<Article>();
		thetaMinusArticles = new HashSet<Article>();
		thetaPlus = new HashMap<Integer, Integer>();
		thetaMinus = new HashMap<Integer, Integer>();
		executorPool = Executors.newFixedThreadPool(NUMTHREADS);
	}
	
	public void run(int initialDocumentID, int numRecommendations) {
		Helper.print("(" + initialDocumentID + ") Setting initial document");
		addInitialDocument(initialDocumentID);
		Helper.print("(" + initialDocumentID + ") Finding seeds for thetaMinus for " + initialDocumentID);
		findThetaMinusSeed();
		
		thetaPlus = calculateTheta(thetaPlusArticles);
		thetaMinus = calculateTheta(thetaMinusArticles);
		
		Helper.print("(" + initialDocumentID + ") Asssigning articles");
		extractCoreArticles();
		Helper.print("(" + initialDocumentID + ") Storing data");
		storeData();
		Helper.print("(" + initialDocumentID + ") Requesting serendipitious documents");
		
		findSerendipitousDocs(numRecommendations);
	}
	
	/***
	 * Iterates over candidates to determine "core" articles that make up the positive and negative LM
	 */
	public void extractCoreArticles() {
		//Randomly iterate over remaining articles and assign them to best fitting a document set
		for (Article a : Helper.shuffleArticleOrder(candidateArticles)) {
			assignArticle(a);
		}
		
		Helper.print("(" + initialArticle.getID() + ") Assigned " + thetaPlusArticles.size() + " documents to D1 and " + thetaMinusArticles.size() + " documents to D2");
//		reestimatePi();
	}

	
	/***
	 * Assigns an article to either the positive or negative LM, depending on which LM has the higher 
	 * probability of generating the article
	 * @param a
	 */
	private void assignArticle(Article a) {
//		double thetaPlusProb = calculateThetaAddedLogProbability(a, thetaPlus);
//		double thetaMinusProb = calculateThetaAddedLogProbability(a, thetaMinus);
		
		double thetaPlusProb = ProbabiliyCalculator.calculateSmoothedLogProability(a, thetaPlus);
		double thetaMinusProb = ProbabiliyCalculator.calculateSmoothedLogProability(a, thetaMinus);
		
		if (thetaPlusProb >= thetaMinusProb) {
			thetaPlusArticles.add(a);
			if (printProbabilityData) Helper.print("Added " + a.getTitle() + " (\"" + a.getTopic() + "\") to D1 with logP " + thetaPlusProb + " over " + thetaMinusProb);
			addToTheta(a, thetaPlus);
		} else {
			thetaMinusArticles.add(a);
			if (printProbabilityData) Helper.print("Added " + a.getTitle() + " (\"" + a.getTopic() + "\") to D2 with logP " + thetaMinusProb + " over " + thetaPlusProb);
			addToTheta(a, thetaMinus);
		}
		
		candidateArticles.remove(a);
	}
	
	

	/***
	 * Calculates the smoothed log probability of the article being generated by theta, i.e. -log P(document | theta). 
	 * The bigger P, the more -log(P) goes to 0
	 * Smoothing here: document is being added to theta
	 * @param article the article for which the probability of being generated by theta shall be calculated
	 * @param theta the theta to use
	 * @return -log P(article)
	 */
	private double calculateThetaAddedLogProbability(Article article, HashMap<Integer, Integer> theta) {
		//n is the number of total word/feature occurrences
		int n = 0;
		//HashMap to store the feature occurrences for the article
		HashMap<Integer, Integer> articleOccurrences = new HashMap<Integer, Integer>(); //map: FeatureID -> count

		//create a smoothed theta
		HashMap<Integer, Integer> thetaSmoothed = new HashMap<Integer, Integer>(theta.size());
		for (int featureID : theta.keySet())  {
			thetaSmoothed.put(featureID, theta.get(featureID));
			n+= theta.get(featureID);
		}
		
		//Then add feature frequencies for the article
		for (ArticleFeature feat : article.getArticleFeatures()) {
			int featureID = feat.getFeatureID();
			if (articleOccurrences.containsKey(featureID)) {
				articleOccurrences.put(featureID, feat.getFrequency() + articleOccurrences.get(featureID));
			} else {
				articleOccurrences.put(featureID, feat.getFrequency());
			}
			
			//and add to the smoothed theta
			if (thetaSmoothed.containsKey(featureID)) {
				thetaSmoothed.put(featureID, feat.getFrequency() + thetaSmoothed.get(featureID));
			} else {
				thetaSmoothed.put(featureID, feat.getFrequency());
			}
			
			n += feat.getFrequency();
		}
		
		int m = articleOccurrences.keySet().size();
		//probability is being calculated by 
		//(n! / (x_1! * ... * x_k!) )* (p_1)^(x_1) * ... * (p_k)^(x_k)
		// = log(n!) - log(x_1!) + ... + log(x_k!) + (x_1)*log(p_1) + ... + (x_k)*log(p_k)
		//WITH SUM(p_i) = 1 AND SUM(k_i) = n
		
		//The best estimator for p_i = (k_i) / SUM_x(k_x) = (k_i) / n
		
		//calculate log(n!)
		double articleProbability = Helper.logFactorial(n);
		
		for (int featureID : articleOccurrences.keySet()) {
			//This is k_i, the number of occurrences of feature i
			int k_i = thetaSmoothed.get(featureID);
			//This is p_i, the probability of feature i  
			double p_i = (k_i * 1.0) / (n + m);
			
			//now, we add log((p_i^(k_i)) / (k_i)!) = log(p_i) * k_i - log(k_i!)
			articleProbability += Math.log(p_i) * k_i - Helper.logFactorial(k_i);
		}
		
		//- log probability!
		return articleProbability * (-1);
		
	}


	/***
	 * Specifies the ID of the initial document
	 */
	protected void addInitialDocument(int initialDocumentID) {
		if (fm.getArticle(initialDocumentID) == null) {
			Helper.printErr("ARTICLE NOT FOUND");
			System.exit(-1);
		}
		
		extractCandidateArticles();
		
		
		initialArticle = findArticle(initialDocumentID, candidateArticles);
		thetaPlusArticles.add(initialArticle);
		
		candidateArticles = fm.getArticlesAsSet();
		//remove the initial article
		candidateArticles.remove(initialArticle);
		//if (printInitialDocumentData) 
		Helper.print("Initial Document is " + initialArticle.getID() + "(" + initialArticle.getTopic() + "): " + initialArticle.getTitle());

		thetaPlus = calculateTheta(thetaPlusArticles);
	}
	
	/***
	 * Finds the seed for the negative LM
	 */
	protected void findThetaMinusSeed() {
		HashSet<Article> minusCandidates = findNonIntersectingCandidates(initialArticle);
		int i = 0;
		for (Article a : Helper.shuffleArticleOrder(minusCandidates)) {
			if (i < NUMSEEDSTHETAMINUS) {
				thetaMinusArticles.add(a);
				candidateArticles.remove(a);
			} else break;

			i++;
		}
	}

	/***
	 * Finds the k most serendipitous Documents
	 * @param numRecommendations determines the k
	 */
	public void findSerendipitousDocs(int numRecommendations) {
		HashMap<Integer, Integer> thetaRed = calculateThetaRed(THETARED_K, thetaPlus);
		
		findSerendipitousDocs(numRecommendations, thetaPlusArticles, thetaRed, thetaPlus);
	}
	
	//theta+ = article 1, theta- = article 2
	public void storeData() {
		if (storeData) DBConnection.getInstance().updateAssignments(initialArticle.getID(), thetaPlusArticles, advancedAlgorithm, dataSlice);
	}

	@Override
	public void assignToMainCore(ArrayList<Integer> mainArticleIDs) {
		for (int articleID : mainArticleIDs) {
			
			Article article = findCandidateArticle(articleID);
			Helper.print("Article is " + article); 
			thetaPlusArticles.add(article);
			addToTheta(article, thetaPlus);
			candidateArticles.remove(article);
		}
	}

	@Override
	public void setRemainingArticlesNonCore() {
		for (Article article : candidateArticles) {
			thetaMinusArticles.add(article);
			addToTheta(article, thetaMinus);
		}
		candidateArticles = new HashSet<Article>();
	}
	
}
